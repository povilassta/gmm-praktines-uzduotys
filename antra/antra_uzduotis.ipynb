{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openimages in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from openimages) (1.5.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from openimages) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from openimages) (2.28.2)\n",
      "Requirement already satisfied: boto3 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from openimages) (1.26.79)\n",
      "Requirement already satisfied: cvdata in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from openimages) (0.0.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from openimages) (4.9.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torchmetrics) (1.24.2)\n",
      "Requirement already satisfied: torch>=1.8.1 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (4.4.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.79 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from boto3->openimages) (1.29.79)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from boto3->openimages) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from boto3->openimages) (1.0.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from cvdata->openimages) (4.7.0.72)\n",
      "Requirement already satisfied: pillow in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from cvdata->openimages) (9.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from pandas->openimages) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from pandas->openimages) (2022.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->openimages) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->openimages) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->openimages) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->openimages) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tqdm->openimages) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->openimages) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openimages torchmetrics tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\povil\\miniconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from openimages.download import download_dataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 40\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading is starting...\n"
     ]
    }
   ],
   "source": [
    "all_transforms = transforms.Compose([transforms.Resize((227,227)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.485, 0.456, 0.406])\n",
    "                                     ])\n",
    "\n",
    "data_dir = \"data\"\n",
    "\n",
    "classes = [\"Person\", \"Food\", \"Animal\"]\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "  os.makedirs(data_dir)\n",
    "\n",
    "print(\"Downloading is starting...\")\n",
    "download_dataset(data_dir, classes, limit=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, images_dir, transform=None):\n",
    "    self.images_dir = images_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    self.class1_files = glob(self.images_dir + \"/{}/images/*.jpg\".format(classes[0].lower()))\n",
    "    self.class2_files = glob(self.images_dir + \"/{}/images/*.jpg\".format(classes[1].lower()))\n",
    "    self.class3_files = glob(self.images_dir + \"/{}/images/*.jpg\".format(classes[2].lower()))\n",
    "\n",
    "    self.class1 = len(self.class1_files)\n",
    "    self.class2 = len(self.class2_files)\n",
    "\n",
    "    self.files = self.class1_files + self.class2_files + self.class3_files\n",
    "\n",
    "    self.labels = np.zeros(len(self.files))\n",
    "    self.labels[self.class1:] = 1\n",
    "    self.labels[self.class1 + self.class2:] = 2 \n",
    "\n",
    "    self.order =  [x for x in np.random.permutation(len(self.labels))]\n",
    "    self.files = [self.files[x] for x in self.order]\n",
    "    self.labels = [self.labels[x] for x in self.order]\n",
    "\n",
    "  def __len__(self):\n",
    "    return (len(self.labels))\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    img_path = self.files[i]\n",
    "\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "            \n",
    "    y = self.labels[i]\n",
    "    return (img, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuralNet(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "full_dataset = CustomDataset(\"./data\", all_transforms)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 0.7527\n",
      "Epoch [2/40], Loss: 0.6625\n",
      "Epoch [3/40], Loss: 0.6068\n",
      "Epoch [4/40], Loss: 0.5730\n",
      "Epoch [5/40], Loss: 0.5500\n",
      "Epoch [6/40], Loss: 0.5357\n",
      "Epoch [7/40], Loss: 0.5010\n",
      "Epoch [8/40], Loss: 0.4945\n",
      "Epoch [9/40], Loss: 0.4612\n",
      "Epoch [10/40], Loss: 0.4190\n",
      "Epoch [11/40], Loss: 0.4461\n",
      "Epoch [12/40], Loss: 0.3615\n",
      "Epoch [13/40], Loss: 0.3949\n",
      "Epoch [14/40], Loss: 0.3114\n",
      "Epoch [15/40], Loss: 0.3745\n",
      "Epoch [16/40], Loss: 0.3153\n",
      "Epoch [17/40], Loss: 0.2845\n",
      "Epoch [18/40], Loss: 0.3220\n",
      "Epoch [19/40], Loss: 0.2072\n",
      "Epoch [20/40], Loss: 0.1803\n",
      "Epoch [21/40], Loss: 0.4005\n",
      "Epoch [22/40], Loss: 0.2287\n",
      "Epoch [23/40], Loss: 0.2512\n",
      "Epoch [24/40], Loss: 0.2558\n",
      "Epoch [25/40], Loss: 0.1145\n",
      "Epoch [26/40], Loss: 0.1332\n",
      "Epoch [27/40], Loss: 0.1318\n",
      "Epoch [28/40], Loss: 0.2004\n",
      "Epoch [29/40], Loss: 0.1540\n",
      "Epoch [30/40], Loss: 0.1638\n",
      "Epoch [31/40], Loss: 0.1903\n",
      "Epoch [32/40], Loss: 0.1048\n",
      "Epoch [33/40], Loss: 0.1275\n",
      "Epoch [34/40], Loss: 0.1434\n",
      "Epoch [35/40], Loss: 0.1798\n",
      "Epoch [36/40], Loss: 0.1387\n",
      "Epoch [37/40], Loss: 0.0873\n",
      "Epoch [38/40], Loss: 0.0609\n",
      "Epoch [39/40], Loss: 0.1054\n",
      "Epoch [40/40], Loss: 0.0279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNeuralNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        labels = labels.to(torch.int64)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "torch.save(model.state_dict(), \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeuralNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        for i, prediction in enumerate(outputs):\n",
    "            predictions.append(prediction.tolist())\n",
    "            temp = np.zeros(3, dtype=int)\n",
    "            temp[int(labels[i].item())] = 1\n",
    "            targets.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Threshold    Accuracy    Precision    Recall     F1\n",
      "-----------  ----------  -----------  --------  -----\n",
      "        0.1       0.642        0.48      0.911  0.629\n",
      "        0.2       0.681        0.513     0.881  0.648\n",
      "        0.3       0.706        0.537     0.858  0.661\n",
      "        0.4       0.725        0.558     0.842  0.671\n",
      "        0.5       0.737        0.574     0.819  0.675\n",
      "        0.6       0.747        0.59      0.792  0.676\n",
      "        0.7       0.76         0.613     0.761  0.679\n",
      "        0.8       0.771        0.638     0.722  0.677\n",
      "        0.9       0.782        0.676     0.664  0.67\n"
     ]
    }
   ],
   "source": [
    "def calculateMetrics(predictions, targets):\n",
    "    thresholds = (x * 0.1 for x in range(1, 10))\n",
    "    metrics = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        accuracy_metric = torchmetrics.classification.MultilabelAccuracy(num_labels = num_classes, threshold = t, average = \"micro\")\n",
    "        accuracy = \"{:.3f}\".format(accuracy_metric(predictions, targets).item())\n",
    "\n",
    "        precision_metric = torchmetrics.classification.MultilabelPrecision(num_labels = num_classes, threshold = t, average = \"micro\")\n",
    "        precision = \"{:.3f}\".format(precision_metric(predictions, targets).item())\n",
    "\n",
    "        recall_metric = torchmetrics.classification.MultilabelRecall(num_labels = num_classes, threshold = t, average = \"micro\")\n",
    "        recall = \"{:.3f}\".format(recall_metric(predictions, targets).item())\n",
    "\n",
    "        f1_metric = torchmetrics.classification.MultilabelF1Score(num_labels = num_classes, threshold = t, average = \"micro\")\n",
    "        f1 = \"{:.3f}\".format(f1_metric(predictions, targets).item())\n",
    "\n",
    "        metrics.append([t, accuracy, precision, recall, f1])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions)\n",
    "targets_tensor = torch.tensor(targets)\n",
    "\n",
    "data = calculateMetrics(predictions_tensor, targets_tensor)\n",
    "col_names = [\"Threshold\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "print(tabulate(data, headers=col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, labels):\n",
    "  img = Image.open(image_path)\n",
    "  img_tensor = all_transforms(img).to(device).unsqueeze(0)\n",
    "\n",
    "  output = model(img_tensor)\n",
    "  pred = output.data.cpu().numpy().argmax()  # Get predicted class number\n",
    " \n",
    "  print('Predicted class: {}'.format(labels[pred]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d521f14f3fb86ce8092594d2edafc35729ef61c3495703c2e6a0376d4c9a6f46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
